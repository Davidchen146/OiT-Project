{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb09b1-6503-4dc6-b9c3-608c3e528fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from oit_helpers import DataPreprocessors,LSTM,createDataLoader,getDataLoader\n",
    "\n",
    "sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858f9c1-0e71-4643-b3ce-03553a213837",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc2f3d-d5d7-4f71-9f3e-ac0f2c2ba05b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a4158-e4f7-4ed4-bcc4-8cfba1805c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './datasets/all-crypto-currencies/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv'\n",
    "data = pd.read_csv(data_location, \n",
    "                    parse_dates=['Timestamp'],\n",
    "                    index_col=\"Timestamp\")\n",
    "no_missing_data = data.dropna(subset=['Close'])\n",
    "dates = pd.to_datetime(no_missing_data.index, unit='s')\n",
    "no_missing_data.index = dates\n",
    "cropped = no_missing_data[no_missing_data.index.year>2016].copy()\n",
    "print(cropped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5b674-5b45-489e-b3a8-43320c74f3f3",
   "metadata": {},
   "source": [
    "## 2. Extract Bitcoin closing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c6ffa-844d-4e5e-bbbd-5ebfdaef961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_close = pd.DataFrame({'Close': cropped.Close.copy()})\n",
    "bit_close.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30026ee-bbeb-4f99-92ec-56ffd3610a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_close.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "569d429a-9f95-407c-9da7-74373191129d",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "(sns.lineplot(x=cropped.index, y=\"Close\", data=cropped).set_title(\"BitCoin Closing Prices\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef567b4-8320-467b-a5a3-1e2b0b0e0020",
   "metadata": {},
   "source": [
    "## 3. Initialize preprocessor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021ce8b-00fc-4443-afce-b89ba8a5101d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our custom preprocessor class\n",
    "dprep = DataPreprocessors(data=bit_close)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211b6c2-4bcf-42bf-9312-93fbe703dd36",
   "metadata": {},
   "source": [
    "## 4. Prepare time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9cc6e4-025d-46f7-b640-cc8d348c5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_data = dprep.original_data.iloc[:,0] # we need a pandas series for the function\n",
    "n_lag = 30 # aka the timestep\n",
    "historical_all,target_all = dprep.create_timeseries_sequences(data=series_data,\n",
    "                                                              timestep=n_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae7929-0939-4af6-93cd-b96f0773fb87",
   "metadata": {},
   "source": [
    "## 5. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db3f15-8e57-42bf-b970-23a13b0daf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(historical_all.shape)\n",
    "print(target_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7928a95-f927-47b3-b134-8e9d1ff54a38",
   "metadata": {},
   "source": [
    "Funky way of doing this, but essentially we are creating train/test datasets, then using the training set to further create train/validate datasets. This leads to **three** datasets: training (for training/tuning); validation (for tuning and assessing performance (while training) on data that \"haven't been seen\"; testing (for final validation).\n",
    "\n",
    "If you play around with this, careful not to make the training set too small. Validation and testing can be smaller, but also shouldn't be too small. Read up on this, or experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60326a5-4b89-4a5a-ae3f-8d6a8a6b1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_train, historical_test, target_train, target_test = (dprep\n",
    "                                                                .train_test_split(historical=historical_all, \n",
    "                                                                                  target=target_all, \n",
    "                                                                                  prop_train=.98,\n",
    "                                                                                  valid_set=False))\n",
    "print(historical_train.shape)\n",
    "print(historical_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561aab78-ee52-4231-a06f-e842ee651ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_train, historical_val, target_train, target_val = (dprep\n",
    "                                                                .train_test_split(historical=historical_train, \n",
    "                                                                                  target=target_train, \n",
    "                                                                                  prop_train=.75,\n",
    "                                                                                  valid_set=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfbfe57-30d4-4592-bc06-d7a5e55069e4",
   "metadata": {},
   "source": [
    "## 6. Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e56e4-c801-4c5e-8ac2-92fb3a7bd1e3",
   "metadata": {},
   "source": [
    "I added a `normalize_data` method. Look at it closely and make sure you understand what's going on. Important: Remember that you don't want to mix information from the training into/from validation/testing data. Scaling data technically gathers information about the data that's being scaled so there's a proper way of doing this:\n",
    "\n",
    "- Gather information from the training data needed for scaling, and transform it\n",
    "- Use that information to also transform the validation/testing data\n",
    "- Note: We're not gathering information from validation/testing data\n",
    "\n",
    "The transformers we use from `scikit-learn` offer intuitive methods to do this: \n",
    "- `fit_transform()` for the training data\n",
    "- `transform()` for validation/testing data\n",
    "\n",
    "Make sure you see how this mechanism is working within the `normalize_data` method I wrote, and how it's being used in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea636d-2668-45ed-ba56-769555c0f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the historical training set to create the scaler and normalize the data to be between 0 and 1\n",
    "historical_train, scaler_ = dprep.normalize_data(historical_train, train = True)\n",
    "# Normalize the validation set and the testing set\n",
    "historical_val, _ = dprep.normalize_data(historical_val, scaler_)\n",
    "historical_test, _ = dprep.normalize_data(historical_test, scaler_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147770e-d716-4d40-a2c1-e9eef109eafb",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bf9d4-0d60-45de-8f76-21c14a65c2e8",
   "metadata": {},
   "source": [
    "## 1. Create dataloaders for training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c284b2-cca5-4304-ae77-9f5423fe71c0",
   "metadata": {},
   "source": [
    "One good practice when using PyTorch is to make use of their `DataLoader` class object. Read about it [here](https://www.geeksforgeeks.org/how-to-use-a-dataloader-in-pytorch/), but simply put...it's a custom class that organizes the data input we feed to torch models, and makes working with big data much more efficient in terms of time and memory.\n",
    "\n",
    "`params` is a dictionary that holds parameters used by the DataLoader object. I've added one (which is simply the default value as an example), but take a look [here](https://pytorch.org/docs/stable/data.html) to see a list of options that you can play around with. Get curious!\n",
    "\n",
    "Follow the trail and see how these parameters impact how PyTorch works with the data/model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ad879-0688-45ce-bc32-d53144f5f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with these params\n",
    "params = {'batch_size':50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46232e38-6f90-4a0a-b4b3-f180077dc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = getDataLoader(historical_train, target_train, params)\n",
    "test_generator = getDataLoader(historical_val, target_val, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aee47f-ae9b-4ef4-b019-8ba92811a957",
   "metadata": {},
   "source": [
    "## 2. Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a229bce-10bd-4ec3-82fe-eb614446ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 100\n",
    "n_layers = 1\n",
    "output_size = 1\n",
    "dropout = 0 #default\n",
    "device = \"cpu\" #only option, unless you have a fancy GPU\n",
    "\n",
    "model = LSTM(input_size = n_lag, \n",
    "             hidden_layer_size = hidden_layer_size, \n",
    "             output_size = output_size,\n",
    "             n_layers = n_layers,\n",
    "             do = dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f04a0-3377-44f8-8800-f63d1aee0335",
   "metadata": {},
   "source": [
    "## 3. Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f975d1c-e2de-4f0e-bbaa-26d1350e0f4c",
   "metadata": {},
   "source": [
    "There are DOZENS of hyperparameters that we can tune in neural networks. Play around with these and try to see how they impact performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff012766-0cc4-4261-a365-478e14c4adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcfb6d-7751-4651-a08a-985696e461b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the mean squred error loss function. \n",
    "loss_fn = nn.MSELoss()\n",
    "# \n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b78362-da47-4b42-a138-a0b1e7d7151a",
   "metadata": {},
   "source": [
    "## 4. Train the model\n",
    "\n",
    "Note that what we're collecting in the lists below aren't the MSE values, but calculated RMSE values instead. Make sure you read about these two and how they are related. Just note that we're using MSE as the loss function, but simply collection RMSE for reporting purposes. Why? Because compared to MSE, RMSE is *in the same scale as the raw data*. Thus, you can interpret error in terms of dollar amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c5fba-679a-4ecd-b54f-f77b96a91a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the losses across epochs\n",
    "train_losses = []\n",
    "valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532e814-852a-4df2-9570-e50e1064983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c2d7b-2737-4bc9-8c94-0d40c1079366",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(xb,yb) in enumerate(training_generator):\n",
    "    if i==0:\n",
    "        orig = xb.size()\n",
    "        sq = xb.unsqueeze(0)\n",
    "        print(\"original input size\",orig)\n",
    "        print(\"unsqueezed input size\",sq.size())\n",
    "        print(\"original target size\",yb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceccb29b-295c-499d-8724-64f890136b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.rand(1,1,30)\n",
    "output = model(input_, 1)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d60675-e01c-4eb8-b1b1-3af18ab0f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb2c3a-eed3-47e4-89d3-7d67a2fb228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training loop over epochs\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # loss at the start of each epoch. Both training and validation starts at 0.\n",
    "    ls = 0\n",
    "    valid_ls = 0\n",
    "    # Train for one epoch\n",
    "    for xb, yb in training_generator:\n",
    "        # Perform the forward pass\n",
    "        # The pytorch.unsqueeze function adds a new dimension of size 1 to the tensor.\n",
    "        ips = xb.unsqueeze(0)\n",
    "        targs = yb\n",
    "        # feed the inputs into the model and get the outputs. We are doing a forward pass\n",
    "        op = model(ips,training_generator.batch_size)\n",
    "\n",
    "        # Backpropagate the errors through the network\n",
    "        optim.zero_grad()\n",
    "        loss = loss_fn(op, targs)\n",
    "        loss.backward()\n",
    "        # the next step in the optimization function\n",
    "        optim.step()\n",
    "        # the cost function, which is just the average of the loss of all the tensors\n",
    "        ls += (loss.item() / ips.shape[1])\n",
    "        \n",
    "    # Check the performance on validation data\n",
    "    for xb, yb in test_generator:\n",
    "        ips = xb.unsqueeze(0)\n",
    "        # use the predict method in the model using input\n",
    "        ops = model.predict(ips, training_generator.batch_size)\n",
    "        vls = loss_fn(ops, yb)\n",
    "        valid_ls += (vls.item() / xb.shape[1])\n",
    "\n",
    "    # Take the square root of the mean square error to see the loss in actual dollar amounts\n",
    "    rmse = lambda x: round(sqrt(x * 1.000), 3)\n",
    "    train_losses.append(str(rmse(ls)))\n",
    "    valid_losses.append(str(rmse(valid_ls)))\n",
    "\n",
    "    # Print the total loss for every tenth epoch\n",
    "    if (epoch % 1 == 0) or (epoch == 1):\n",
    "        print(f\"Epoch {str(epoch):<4}/{str(n_epochs):<4} | Train Loss: {train_losses[-1]:<8} | Validation Loss: {valid_losses[-1]:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda2c19-ef31-44dc-8de5-d742686e9c59",
   "metadata": {},
   "source": [
    "Notes for this week:\n",
    "    -Experiment with:\n",
    "        -batch size (speed? performance?)\n",
    "        -different loss functions\n",
    "        -different cryptocurrency data\n",
    "    -Make a graph visualizing the loss / accuracy of predictions.\n",
    "    -How to make train/test splits for time-series data\n",
    "        -Right now, the training data has never seen the big spikes near the end.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8296e-09cd-4b39-8fe9-b94b0024efa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
