{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e09452-1760-4e43-a8d2-247e05c1b7d1",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Our goal is to forecast Bitcoin (closing) prices using historical data. To accomplish this, we'll use a long short term memory (LSTM) model, a special kind of recurrent neural network (RNN) that is often used to model time series data.\n",
    "\n",
    "Before training a model, we must first perform a few data preprocessing steps that are either required by the model (e.g., all data must be represented numerically), or improve the performance of the model (e.g., normalization). This notebook covers the following preprocessing steps:\n",
    "\n",
    "1. Extracting relevant data from original dataset\n",
    "2. Preparing time series data for LSTM\n",
    "3. Creating training and test sets for model training\n",
    "4. Normalizing the data\n",
    "5. Reshaping data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0dbb1-5e1c-4158-bfce-b319448ad490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "from oit_helpers import DataPreprocessors,LSTM\n",
    "\n",
    "sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9b830-4207-4d40-afd3-e50965303e25",
   "metadata": {},
   "source": [
    "## 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c51f9-f8c7-42a3-8e58-bf7f6c8bf667",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './datasets/all-crypto-currencies/crypto-markets.csv'\n",
    "data = pd.read_csv(data_location, \n",
    "                   parse_dates=['date'], \n",
    "                   index_col=\"date\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95cc2b7-21a2-4506-ae68-cf062030297b",
   "metadata": {},
   "source": [
    "## 2. Extract Bitcoin closing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0b540-0768-4750-bee2-3a6540665fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_data = (data\n",
    "            .query('slug==\"bitcoin\"')\n",
    "            .copy()[['close']])\n",
    "bit_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf87342-8bb5-42da-96ea-17dba56c6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "(sns.lineplot(x=bit_data.index, y=bit_data.close)\n",
    " .set_title(\"Closing price of Bitcoin\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec22367-fba9-4b1f-9d78-def97d88772e",
   "metadata": {},
   "source": [
    "## 3. Initialize preprocessor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f813564-af54-4246-aa4f-ec0fcc111f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our custom preprocessor class\n",
    "dprep = DataPreprocessors(data=bit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce4d5f-8810-4ce4-93be-1e4785d05dcd",
   "metadata": {},
   "source": [
    "## 3. Prepare time series data\n",
    "\n",
    "A time series is a sequence of data points, equally spaced through time. Do you see why our Bitcoin prices constitute a time series? At what interval are they measured? \n",
    "\n",
    "When modeling time series data, we often use a \"sliding window\" in which we use a number of consecutive data points to predict the next data point (sometimes more) in the series. The number of consecutive data points is referred to as the **timestep**. Consider a timestep equal to `step = 90` for our data. Our sliding window would, for example, use the first 90 closing prices to predict the 91st closing price. Next, we'd take the 2nd through 91st set of observations to predict the 92nd data point, etc. \n",
    "\n",
    "Explore the `create_timeseries_sequences()` function included in `oit_helpers.py` and make sure you see what's going on. Slice and dice the original data and compare it to the output of the function. Specifically, ensure you see how the predictor and predicted variables are formed, and where they fall in relation to the original dataset.\n",
    "\n",
    "**Task**\n",
    "\n",
    "Read about timesteps and sliding windows for time series data. Research how to choose the timestep -- what implications it has for time series models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875168b-0bd3-46d1-b03b-b6fb5f0eb949",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_data = dprep.original_data.iloc[:,0] # we need a pandas series for the function\n",
    "historical_all,target_all = dprep.create_timeseries_sequences(data=series_data,\n",
    "                                                              timestep=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a8dae-30d6-4614-9b32-7bfdeefb6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(historical_all.shape)\n",
    "print(target_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029aa56-998b-4e3d-8e01-3457d2e17998",
   "metadata": {},
   "source": [
    "## 4. Train test split\n",
    "\n",
    "Ahead of training a predictive model, you will almost always perform a train test split on the original data. Essentially, you split the data into a **training set** that you will use to build the model, and a **test set** that you use in the final evaluation of your trained model. It is very important that once you create a test set, you essentially forget about it until the very end. Even normalizing the training and test sets together is technically incorrect, since you're allowing information from the training set to *leak* into your test set.\n",
    "\n",
    "A decision to make regarding a train test split is the proportion of the original data that you use for each set. Generally, we need a larger sample for the training set, while ensuring that the test set isn't too small. When you have big data, you'll commonly see a 90%/10% split.\n",
    "\n",
    "**Task**\n",
    "\n",
    "Read about train/test splits and why they're important for building predictive models. Pay close attention to sizing recommendations and play around with the proportion supplied to `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8088f-14eb-4bd0-8fad-fbd3a26b5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_train, historical_test, target_train, target_test = (dprep\n",
    "                                                                .train_test_split(historical=historical_all, \n",
    "                                                                                  target=target_all, \n",
    "                                                                                  prop_train=.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd8c05-0dda-4201-85fa-43d5ab996469",
   "metadata": {},
   "source": [
    "## 5. Normalize the data\n",
    "\n",
    "Data normalization is usually performed because it *normalizes* the scales of different variables. We're only using 1, but it's still a good practice. Plus, normalization is very helpful when working with neural networks in that it makes training more stable. A canonical choice is min-max normalization. \n",
    "\n",
    "**Task**\n",
    "\n",
    "Write some code that normalizes the training data using the min-max algorithm. Once you work out a solution, add it as a new method to the DataPreprocessors class.\n",
    "\n",
    "**Tips**\n",
    "* Read about [min-max scaling](https://towardsdatascience.com/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79)\n",
    "* Sklearn provides a custom class for min-max normalization `sklearn.preprocessing import MinMaxScaler` (already imported)\n",
    "* `MinMaxScaler` requires 2D data, so you will have to reshape `target_train` using `target_train.reshape(-1,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ded50-486d-4ab7-90ea-c8736c78d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bf52c-25a1-4b50-a062-01d4daadad13",
   "metadata": {},
   "source": [
    "## 6. Save your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8db665-6a5f-4259-8dd5-5f9f1cbfc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data_object.pkl','wb') as f:\n",
    "    pickle.dump(dprep, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
